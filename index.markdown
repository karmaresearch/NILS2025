---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

![VLDB 2025](https://vldb.org/2025/img/vldb2025_logo_simple.png)

# 1st Workshop on New Ideas for Large-Scale Neurosymbolic Learning Systems (LS-NSL) 
<i>Co-located with [VLDB 2025](https://vldb.org/2025/), September 5th, 2025.</i>

## About

Deep learning has been a striking success in various fields from engineering and science. However, the criticism against it is getting bigger as scientists and practitioners apply it more broadly. Neurosymbolic learning (NSL) vows to transform deep learning by combining the strong induction capabilities of neural models with rigorous deduction from symbolic knowledge representation and reasoning techniques. Despite that NSL has shown its potential in different application domains, including image and video understanding, natural language processing, and data management, several questions remain open regarding whether current techniques are mature enough to be applied to large-scale, real-world problems. 

This workshop aims to:
- Identify key large-scale, real-world scenarios from different domains, such as computer vision and data management, that can benefit from NSL techniques. 
- Identify key techniques from the database literature that could enhance NSL techniques for training and inference.  
- Identify new theoretical and engineering challenges that arise when integrating deep networks with symbolic systems and propose solutions towards overcoming them. 
- Discuss scalable techniques for training deep networks using symbolic solvers. 
- Investigate benchmarks across different application domains to assess the strengths of NSL techniques in runtime efficiency, task-specific accuracy, and other aspects.

## Topics of Interest

The topics of interest include (but are not limited to):
- Large-scale NSL applications, e.g., from computer vision, natural language processing, and data management.
- Scalable integration of deep networks with symbolic systems, such as logic programs, or combinatorial solvers.
- Scalable techniques to train deep networks subject to symbolic constraints or logical theories.
- New NSL architectures and semantics.
- Uncertain databases and logic programs.
- Query answering via transformers and graph neural networks.
- Data management over new hardware.
- New forms of databases, e.g., databases to store tensor data.
- Database creation and querying via machine learning.
- NSL benchmarks.

## Call For Contributions

We welcome <strong>regular papers (up to eight pages, including the bibliography)</strong> that present complete novel research outcomes not previously presented elsewhere and <strong>extended abstracts (up to four pages, including the bibliography)</strong> on preliminary results that can trigger discussions. We also welcome <strong>papers accepted by VLDB 2025 or other recent top-tier AI, machine learning, and database venues</strong>. The submissions will be single-blind and the conflicts of interest will be handled by adhering to the “Conflict and Authorship” guidelines adopted by VLDB 2025.

At least one author of each accepted paper is expected to register to the workshop and give an oral presentation.


## Important Dates

- Paper submission: <strong>Friday, May 30th, 2025.</strong>
- Notification of acceptance: <strong>Friday, June 27th, 2025.</strong>
- Camera-ready submission: <strong>Friday, July 11th, 2025.</strong>
- Workshop Date: <strong>September 5th, 2025.</strong>

## Keynotes

<strong><i>Dan Roth, University of Pennsylvania and Oracle</i></strong>

<figure class="image">
<img src="assets/Roth_dan.jpg" alt="Dan Roth" width=150>
</figure>

<strong>Title:</strong> TBA

<strong>Bio:</strong> Dan Roth is the Eduardo D. Glandt Distinguished Professor at the Department of Computer and Information Science, University of Pennsylvania and the Chief AI Scientist at Oracle. Until June 2024 Dan was a VP/Distinguished Scientist at AWS AI. In his role at AWS Roth led over the last three years the scientific effort behind the first-generation Generative AI products from AWS, including Titan Models, Amazon Q efforts, and Bedrock, from inception until they became generally available. 

Dan is a Fellow of the AAAS, ACM, AAAI, and ACL. In 2017, Dan was awarded the John 
McCarthy Award; he was recognized for “for major conceptual and theoretical advances in the modeling of natural language understanding, machine learning, and reasoning.” He has published broadly in natural language processing, machine learning, knowledge representation and reasoning, and learning theory, was the Editor-in-Chief of the Journal of Artificial Intelligence Research (JAIR) and has served as a Program Chair and Conference Chair for the major conferences in his research areas. Roth has been involved in several startups; most recently he was a co-founder and chief scientist of NexLP, a startup that leverages the latest advances in Natural Language Processing, Cognitive Analytics, and Machine Learning in the legal and compliance domains. NexLP was acquired by Reveal. Dan received his B.A Summa cum laude in Mathematics from the Technion, Israel and his Ph.D. in Computer Science from Harvard University in 1995.

<strong><i>Jeff Pan, University of Edinburgh and Huawei Labs</i></strong>

<figure class="image">
<img src="assets/Jeff_Pan.jpg" alt="Jeff Pan" width=150>
</figure>

<strong>Title:</strong> Decoding the Interaction of Symbolic and Parametric Knowledge
 
<strong>Abstract:</strong> Large Language Models (LLMs) have taken Knowledge Representation – and the world – by storm. This inflection point marks a shift from symbolic knowledge representation to a renewed focus on the hybrid representation of both symbolic knowledge and parametric knowledge. This is a big step for the field of Knowledge Representation. In this talk, I will briefly introduce some initial findings in such a big step. If time allows, I will also  speculate on opportunities and visions that the renewed focus brings.

<strong>Bio:</strong> Jeff Pan is professor of knowledge computing in the School of Informatics at the University of Edinburgh. He is a chair of the Knowledge Graphs group at the Alan Turing Institute. He is the Chief Editor and main author of the first book on Knowledge Graph. Recently, he teamed up with many group leaders in the world on a visionary paper on large language models and knowledge graphs ([LINK](https://drops.dagstuhl.de/storage/08tgdk/tgdk-vol001/tgdk-vol001-issue001/TGDK.1.1.2/TGDK.1.1.2.pdf))

## Program Committee

- <strong>Victor Gutierrez Basulto,</strong> Cardiff University
- <strong>Vaishak Belle,</strong> University of Edinburgh
- <strong>Angela Bonifati,</strong> Lyon 1 University
- <strong>Gianluca Cima,</strong> Sapienza University of Rome
- <strong>Floris Geerts,</strong> University of Antwerp
- <strong>Christoph Haase,</strong> University of Oxford
- <strong>Ziyang Li,</strong> University of Pennsylvania
- <strong>Ankur Mali,</strong> University of South Florida
- <strong>Nikos Ntarmos,</strong> Huawei Labs
- <strong>Hai Pham,</strong> Samsung AI
- <strong>Ernesto Jimenez Ruiz,</strong> City St George's, University of London
- <strong>Luciano Serafini,</strong> Fondazione Bruno Kessler
- <strong>Gerardo I. Simari,</strong> Universidad Nacional del Sur

## Organizers

- [Efthymia (Efi) Tsamoura](), <i>moving to</i> Huawei Labs
- [Pablo Barceló](https://pbarcelo.ing.uc.cl/), Universidad Católica de Chile
- [Jacopo Urbani](https://www.jacopourbani.it), Vrije Universiteit Amsterdam

<i>For any questions, please do reach out to Efi at efthymia.tsamoura@gmail.com.</i>
